# music-recommendation-based-on-facial-emotion-recognition
We propose a new approach for playing music automatically using facial emotion. Most of the existing approaches involve playing music manually, using wearable computing devices, or classifying based on audio features. Instead, we propose to change the manual sorting and playing. We have used a Convolutional Neural Network for emotion detection.
For music recommendations, Pygame & Tkinter are used.
Our proposed system tends to reduce the computational time involved in obtaining the
results and the overall cost of the designed system, thereby increasing the system’s
overall accuracy. Testing of the system is done on the FER2013 dataset. Facial expressions are captured using an inbuilt camera. Feature extraction is performed on input
face images to detect emotions such as happy, angry, sad, surprise, and neutral. Automatically music playlist is generated by identifying the current emotion of the
user. 

 **Introduction**
Many of the studies in recent years admit that humans reply and react to music and this music has a high impression on the
activity of the human brain. In one examination of the explanations why people hear music, researchers discovered that music played a crucial role in relating arousal and mood. Two of the most important functions of music are it is ability is participants rated to help them achieve a good mood and become more self-aware. Musical preferences have been demonstrated
to be highly related to personality traits and moods

. **Problem Definition**
Develop a system that presents a cross-platform music player, which recommends music based on the real-time mood of the
user through a web camera using Machine Learning Algorithms.

. **Proposed System Overview**
The proposed system benefits us to present interaction between the user and the music player. The purpose of the system is to capture the face properly with the camera. Captured images are fed into the Convolutional Neural Network
which predicts the emotion. Then emotion derived from the captured image is used to get a playlist of songs. The main
aim of our proposed system is to provide a music playlist automatically to change the user's moods, which can be happy,
sad, natural, or surprised. The proposed system detects the emotions, if the topic features a negative emotion , then a
selected playlist is going to be presented that contains the foremost suitable sorts of music that will enhance th e mood
of the person positively. Music recommendation based on facial emotion recognition contains four modules.
• Real-Time Capture: In this module, the system is to capture the face of the user correctly
• Face Recognition: Here it will take the user's face as input. The convolutional neural network is programmed to
evaluate the features of the user image.
• Emotion Detection: In this section extraction of the features of the user image is done to detect the emotion and
depending on the user's emotions, the system will generate captions.
• Music Recommendation: Song is suggested by the recommendation module to the user by mapping their emotions
to the mood type of the song.

**Methodology**
5.1 Database Description
We built the Convolutional Neural Network model using the Kaggle dataset. The database is FER2013 which is split into
two parts training and testing dataset. The training dataset consists of 24176 and the testing dataset contains 6043 images. There are 48x48 pixel grayscale images of faces in the dataset. Each image in FER-2013 is labeled as one of five
emotions: happy, sad, angry, surprise, and neutral. The faces are automatically registered so that they are more or less
centered in each image and take up about the same amount of space. The images in FER-2013 contain both posed and
unposed headshots, which are in grayscale and 48x48 pixels.
 The FER-2013 dataset was created by gathering the results of a Google image search of every emotion and synonyms
of the emotions. FER systems being trained on an imbalanced dataset may perform well on dominant emotions such as
happy, sad, angry, neutral, and surprised but they perform poorly on the under-represented ones like disgust and fear.
Usually, the weighted-SoftMax loss approach is used to handle this problem by weighting the loss term for each emotion
class supported by its relative proportion within the training set. However, this weighted-loss approach is predicated on
the SoftMax loss function, which is reported to easily force features of various classes to stay apart without listening to
intra-class compactness

 **Face Detection**
Face detection is one of the applications which is considered under computer vision technology. This is the process in
which algorithms are developed and trained to properly locate faces or objects in object detection or related system in images. This detection can be real-time from a video frame or images. Face detection uses such classifiers, which are
algorithms that detect what's either a face (1) or not a face (0) in an image. Classifiers are trained to detect faces using
numbers of images to get more accuracy. OpenCV uses two sorts of classifiers, LBP (Local Binary Pattern) and Haar Cascades. A Haar classifier is used for face detection where the classifier is trained with pre-defined varying face data which
enables it to detect different faces accurately. The main aim of face detection is to spot the face within the frame by reducing external noises and other factors

**Feature Extraction**
While performing feature extraction, we treat the pre-trained network that is a sequential model as an arbitrary feature extractor. Allowing the input image to pass on it forward, stopping at the pre-specified layer, and taking the outputs of that
layer as our features. Starting layers of a convolutional network extract high-level features from the taken image, so use only
a few filters

 **Emotion Detection**
Convolution neural network architecture applies filters or feature detectors to the input image to get the feature maps or
activation maps using the Relu activation function [11]. Feature detectors or filters help in identifying various features present in the image such as edges, vertical lines, horizontal lines, bends, etc. After that pooling is applied over the feature maps
for invariance to translation. Pooling is predicted on the concept that once we change the input by a touch amount, the
pooled outputs don’t change. We can use any of the pooling from min, average, or max. But max-pooling provides better
performance than min or average pooling. Flatten all the input and giving these flattened inputs to a deep neural network
which are outputs to the class of the object.

The class of the image will be binary, or it will be a multi-class classification for identifying digits or separating various apparel items. Neural networks are as a black box, and learned features in a Neural Network are not interpretable. So basically,
we give an input image then the CNN model returns the results [10]. Emotion detection is performed by loading the model
which is trained by weights using CNN. When we take the real-time image by a user then that image was sent to the
pre-trained CNN model, then predict the emotion and adds the label to the image.

**Music Recommendation Module**
We created a database for Bollywood Hindi songs. It consists of 100 to 150 songs per emotion. As we all know music is undoubtedly involved in enhancing our mood. So, suppose a user is sad then the system will recommend such a music playlist
which motivates him or her and by this automatic mood will be delighted.

**Conclusion**
A thorough review of the literature tells that there are many approaches to implement Music Recommender System. A study
of methods proposed by previous scientists and developers was done. Based on the findings, the objectives of our system
were fixed. As the power and advantages of AI-powered applications are trending, our project will be a state-of-the-art
trending technology utilization. In this system, we provide an overview of how music can affect the user's mood and how to
choose the right music tracks to improve the user's moods. The implemented system can detect the user's emotions. The
emotions that the system can detect were happy, sad, angry, neutral, or surprised. After determining the user’s emotion, the
proposed system provided the user with a playlist that contains music matches that detected the mood. 
